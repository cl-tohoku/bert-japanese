{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["BERT_BASE_DIR = '/Users/m-suzuki/work/japanese-bert/jawiki-20190901/mecab-ipadic-bpe-32k/'"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"I1009 15:53:38.023083 4540030272 file_utils.py:39] PyTorch version 1.2.0 available.\nI1009 15:53:38.439059 4540030272 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"}],"source":["import torch\n","from transformers import BertForMaskedLM\n","from tokenization import MecabBertTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["tokenizer = MecabBertTokenizer(vocab_file=f'{BERT_BASE_DIR}/vocab.txt')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["text = '今日は朝食に[MASK]を焼いて食べました。'"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["token_ids = tokenizer.encode(text, add_special_tokens=True)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":"[2, 3412, 9, 584, 29064, 7, 4, 11, 16755, 16, 2921, 3926, 10, 8, 3]"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["token_ids"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["tokens = tokenizer.convert_ids_to_tokens(token_ids)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":"['[CLS]',\n '今日',\n 'は',\n '朝',\n '##食',\n 'に',\n '[MASK]',\n 'を',\n '焼い',\n 'て',\n '食べ',\n 'まし',\n 'た',\n '。',\n '[SEP]']"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokens"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["token_ids = torch.tensor([token_ids])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([[    2,  3412,     9,   584, 29064,     7,     4,    11, 16755,    16,\n          2921,  3926,    10,     8,     3]])"},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["token_ids"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"I1009 15:53:57.262381 4540030272 configuration_utils.py:148] loading configuration file /Users/m-suzuki/work/japanese-bert/jawiki-20190901/mecab-ipadic-bpe-32k/config.json\nI1009 15:53:57.263567 4540030272 configuration_utils.py:168] Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 2,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"pruned_heads\": {},\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 32000\n}\n\nI1009 15:53:57.266280 4540030272 modeling_utils.py:334] loading weights file /Users/m-suzuki/work/japanese-bert/jawiki-20190901/mecab-ipadic-bpe-32k/pytorch_model.bin\nI1009 15:53:59.724687 4540030272 modeling_utils.py:408] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"}],"source":["model = BertForMaskedLM.from_pretrained(BERT_BASE_DIR)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["predictions, = model(token_ids)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["_, top10_pred_ids = torch.topk(predictions, k=10, dim=2)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":"tensor([[[    6,  3926,     8,    10,   786,  8790,    73,   105,   584,    16],\n         [ 3412,   732, 12050,  7702, 18337,  6011,  8626,  5824,  1322,  8790],\n         [    9,     5,     6,    28,   126, 28448,    73,    40,   226, 15642],\n         [  584,   381,  5106, 10772,  3467, 28948,   174,   109,   814,   310],\n         [29064, 31314,  7171,   757, 30224,   126, 29011, 30108, 28779, 28946],\n         [    7,    50,     9,    28,    11,     5,    13,     6,    12,    23],\n         [ 3443,  3030,     1, 10666, 24156, 19551,  2098,  4201, 19335,  9589],\n         [   11,  3030,    13, 14471,    12,     6,    14,    16, 29620,  6274],\n         [16755,  6274, 24301, 15979,  3290, 24615,  9913, 28413,  2921,  2119],\n         [   16,   887,    12,    10,  3287,    11,     6,   807, 28453, 28454],\n         [ 2921,    21,  6141,  5113,  1158,  2604,  1258,   323,  5328,  3272],\n         [ 3926, 13259,  2554,  6771,  3061,    15, 17066, 18760, 12727,  1158],\n         [   10,    16,     8,  2554,  3926,    38,  7428,    26,    64,    15],\n         [    8,    10,    16,    38,   142,   261,    61,    36,    24,   659],\n         [    8,    10,    16,    38,    61,   142,   261,    36,    24,   659]]])"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["top10_pred_ids"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['[CLS]'] ['、', 'まし', '。', 'た', 'ご', 'いつも', 'お', 'また', '朝', 'て']\n['今日'] ['今日', '今', '明日', '我々', '今年', '昔', '今度', 'きょう', '私', 'いつも']\n['は'] ['は', 'の', '、', 'も', 'まで', '##は', 'お', 'から', 'より', 'はや']\n['朝'] ['朝', '正', '昼', '早朝', '夕', '##朝', '前', '上', '父', '表']\n['##食'] ['##食', '##餐', '食事', '食', '##卓', 'まで', '##接', '##菜', '##身', '##先']\n['に'] ['に', 'として', 'は', 'も', 'を', 'の', 'と', '、', 'で', '(']\n['[MASK]'] ['パン', '肉', '[UNK]', '鶏', '豚肉', 'ジャガイモ', '魚', '卵', '牛肉', '豚']\n['を'] ['を', '肉', 'と', '##焼き', 'で', '、', 'が', 'て', '##肉', '焼き']\n['焼い'] ['焼い', '焼き', '焼く', '焼け', '作っ', '焼か', '買っ', '削っ', '食べ', '焼']\n['て'] ['て', 'ながら', 'で', 'た', 'たら', 'を', '、', 'たり', '##で', '##て']\n['食べ'] ['食べ', 'い', '食べる', 'くれ', '始め', '作り', '来', 'き', '過ごし', '使い']\n['まし'] ['まし', 'でし', 'ます', 'ませ', 'です', 'し', 'ましょ', 'なさい', 'だし', '始め']\n['た'] ['た', 'て', '。', 'ます', 'まし', '」', 'てる', 'さ', '』', 'し']\n['。'] ['。', 'た', 'て', '」', '.', ')。', '『', '「', ')', '!']\n['[SEP]'] ['。', 'た', 'て', '」', '『', '.', ')。', '「', ')', '!']\n"}],"source":["for correct_id, pred_ids in zip(token_ids[0], top10_pred_ids[0]):\n","    correct_token = tokenizer.convert_ids_to_tokens([correct_id.item()])\n","    pred_tokens = tokenizer.convert_ids_to_tokens(pred_ids.tolist())\n","    print(correct_token, pred_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}